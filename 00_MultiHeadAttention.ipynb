{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "\n",
    "En este notebook vamos a seguir la implementación de [LabML](https://nn.labml.ai/transformers/mha.html) para intentar entender cómo funciona este mecanismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareForMultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model, # Dimensión de entrada\n",
    "                 heads, # Cantidad de \"cabezas\" en paralelo\n",
    "                 d_k, # Dimensión de salida\n",
    "                 bias):\n",
    "        \"\"\"\n",
    "        Utilizaremos una capa lineal para proyectar los (Key, Value, Query)\n",
    "        Tenemos que tener tantas salidas como cabezas*dimensión de salida.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(PrepareForMultiHeadAttention, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, heads*d_k, bias=bias)\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        En el paso hacia delante, lo que tenemos que hacer es proyectar las entradas\n",
    "        y separarlo en las cabezas.\n",
    "\n",
    "        La entrada puede tener dimensiones [seq_len, batch, d_model] o [batch, d_model],\n",
    "        así que aplicaremos la transformación a la última dimensión (d_model).\n",
    "        Esto quiere decir que la salida de cada cabeza tiene que tener dimensiones\n",
    "        [seq_len, batch, heads, d_k] o [batch, heads, d_k].\n",
    "        \"\"\"\n",
    "        \n",
    "        head_shape = x.shape[:-1]\n",
    "        x = self.linear(x)\n",
    "        x = x.view(*head_shape, self.heads, self.d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Aquí es dónde vamos a calcular la *scaled multi-head attention* para cualquier tupla `(query, key, value)`:\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = \\underset{seq}{softmax\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)}\n",
    "$$\n",
    "\n",
    "El producto escalar entre $Q$ y $K$ es una medida de similitud, por lo que esta operación se puede entender como buscar la `key` que más se parece a la `query` y obtener su `value`. Esta formulación se asemeja bastante a una búsqueda en una base de datos, donde introducimos aquello que queremos buscar (`query`) y se nos devuelven los elementos de la base de datos (`value`) cuyo indentificador `key` se parece más a la `query` inicial.\n",
    "\n",
    "> También se puede hacer una analogía con introducir una `query` en un diccionario de Python y obtener los valores cuya `key` se parece más a la `query` introducida. \n",
    "\n",
    "Antes de aplicar la función $softmax$, el producto escalar de escala por un factor $\\frac{1}{\\sqrt{d_k}}$ para evitar que los valores muy grandes del producto escalar den gradientes muy pequeños cuando $d_k$ es grande. Esta $softmax$ se aplica en la dirección de la secuencia (o tiempo), es decir, nos sirve para pesar los diferentes momentos temporales. **(creo)**\n",
    "\n",
    "> El resultado de la función $softmax$ tiene que sumar 1, por lo que, si $d_k$ es grande, este 1 hay que repartirlo entre muchos sumandos. Si hay un elemento que es mucho más grande que el resto, habrá elementos que sean prácticamente 0 y podríamos tener problemas con el gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 heads, # Cantidad de cabezas\n",
    "                 d_model, # Dimensión de entrada\n",
    "                 dropout_prob=0.1, # Probabilidad del Dropout\n",
    "                 bias=True):\n",
    "        \n",
    "        ## En `PrepareForMultiHeadAttention` la dimensión de salida de las proyecciones\n",
    "        ## lineales es d_k*heads, por lo que ahora tenemos que definir d_k = d_model//heads\n",
    "        ## para que las proyecciones mantengan la dimensionalidad.\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "\n",
    "        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1) # Yo a priori habría dicho que la dimensión de la secuencia era la 0\n",
    "        self.output = nn.Linear(d_model, d_model) # Hay una capa lineal al final de todo el proceso\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        self.scale = 1 / torch.sqrt(self.d_k) # Factor de escala\n",
    "\n",
    "        self.attn = None # Se guardan las atenciones por lo que pueda pasar\n",
    "\n",
    "    def get_scores(self,\n",
    "                   query, # Vector Q\n",
    "                   key): # Vector K\n",
    "        \"\"\"\n",
    "        Aquí es donde calcularemos el producto escalar entre los vectores Q y K \n",
    "        para obtener una medida de su similitud. Una forma fácil de hacerlo es \n",
    "        utilizando `torch.einsum`.\n",
    "        \"\"\"        \n",
    "\n",
    "        return torch.einsum('ibhd, jbhd -> ijbh', query, key)\n",
    "    \n",
    "    def prepare_mask(self,\n",
    "                     mask, # Tiene shape [seq_len_q, seq_len_k, batch_size]\n",
    "                     query_shape, # Shape del vector Q\n",
    "                     key_shape): # Shape del vector K\n",
    "        \"\"\"\n",
    "        Esta función nos sirve para asegurarnos de que la shape de la máscara\n",
    "        se corresponde con las shapes de los vectores Q y K.\n",
    "\n",
    "        Lo que nos indica la máscara es si la query i tiene acceso a la key j\n",
    "        en el batch b. (mask[i,j,b] = 0/1)\n",
    "        \"\"\"\n",
    "        assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n",
    "        assert mask.shape[1] == key_shape[0]\n",
    "        assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
    "\n",
    "        ## Como queremos aplicar la misma máscara a todas las cabezas, \n",
    "        ## añadimos una dimensión al final para que se aplique broadcasting.\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self,\n",
    "                *,\n",
    "                query, # Vector Q []\n",
    "                key, # Vector K\n",
    "                value, # Vector V\n",
    "                mask = None): # Máscara\n",
    "        \n",
    "        ## Extraemos la longitud de la secuencia y el tamaño del batch\n",
    "        seq_len, batch_size, _ = query.shape\n",
    "\n",
    "        ## En caso de haber una máscara, la preparamos\n",
    "        if mask is not None:\n",
    "            self.prepare_mask(mask, query.shape, key.shape)\n",
    "\n",
    "        ## Proyectamos los vectores Q, K y V antes de pasar por la MultiHeadAttention\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        ## Obtenemos los scores de atención y los escalamos\n",
    "        scores = self.get_scores(query, key)\n",
    "        scores = scores*self.scale\n",
    "\n",
    "        ## Aplicamos la máscara en caso de haberla\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "\n",
    "        ## Aplicamos la función softmax para obtener la atención\n",
    "        attn = self.softmax(scores)\n",
    "\n",
    "        ## Y aplicamos también el Dropout\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        ## Calculamos el producto escalar con el vector V\n",
    "        x = torch.einsum('ijbh, jbhd -> ibhd', attn, value)\n",
    "\n",
    "        ## Guardamos attn por lo que pueda pasar\n",
    "        self.attn = attn.detach()\n",
    "\n",
    "        ## Finalmente, concatenamos las salidas de todas las cabezas\n",
    "        ## y pasamos el vector resultante por la capa de salida\n",
    "        x = x.reshape(seq_len, batch_size, -1)\n",
    "\n",
    "        return self.output(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
